{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook demos the usage of how to run GPTQ on a quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmx.compressor.modeling.hf import pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"d-matrix/opt\",\n",
    "    revision=\"opt-125m\",\n",
    "    dmx_config=\"BASELINE\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",  # enabling model parallel on multi-GPU nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block configures the model to the right format.\n",
    "\n",
    "xxx_format takes a single value.\n",
    "\n",
    "input_formats takes a list or a dictionary. When a list is passed, the formats will be set in the order of the castTos within input_casts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmx.compressor.modeling import DmxConfigRule,nn\n",
    "format = \"MXFP8[E4M3]{64}\"\n",
    "weight_format = \"MXINT4{64}\"\n",
    "rules = (\n",
    "    DmxConfigRule(\n",
    "        module_types=(nn.Linear,),\n",
    "        module_config=dict(\n",
    "            input_formats=[format],  # option 1\n",
    "            # input_formats = {\"input_cast\": format} # option 2\n",
    "            weight_format=weight_format,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "# configure model based on rules\n",
    "pipe.model.configure(None, *rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: if you are using INT formats that requires calibration, please refer to calibration.ipynb on how to do quantization calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate before gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = pipe.evaluate(\n",
    "    \"d-matrix/dmx_perplexity\",\n",
    "    dataset=\"wikitext\",\n",
    "    dataset_version=\"wikitext-2-raw-v1\",\n",
    ")\n",
    "print(\"before gptq:\",metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define layers to run gptq on. \n",
    "\n",
    "Note that at least a forward pass of the model needs to be run before this point so that dmxModules exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_gptq = {lname: layer for lname, layer in pipe.model.named_dmx_modules()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters for gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTQ_HYPERPARAMS = dict(\n",
    "    block_size=128,\n",
    "    microblock_size=64, # if weight format is blocked, microblock_size needs to be same as block size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with pipe.model.optimal_brain_compressing(\n",
    "    layers_to_gptq.items(),\n",
    "    microblock_size=GPTQ_HYPERPARAMS['microblock_size'],\n",
    "    block_size=GPTQ_HYPERPARAMS['block_size'],\n",
    "), torch.no_grad():\n",
    "    pipe.do_forward_on(\n",
    "      dataset = \"wikitext\",dataset_version=\"wikitext-2-raw-v1\",column_name = \"text\",dataset_split=\"train\",num_samples=10\n",
    "    )\n",
    "metric = pipe.evaluate(\n",
    "    \"d-matrix/dmx_perplexity\",\n",
    "    dataset=\"wikitext\",\n",
    "    dataset_version=\"wikitext-2-raw-v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate after gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = pipe.evaluate(\n",
    "    \"d-matrix/dmx_perplexity\",\n",
    "    dataset=\"wikitext\",\n",
    "    dataset_version=\"wikitext-2-raw-v1\",\n",
    ")\n",
    "print(\"after gptq:\",metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
