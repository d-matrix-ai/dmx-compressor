{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook demos the usage of how to create and configure a quantized Huggingface model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Huggingface text generation pipeline with a model. \n",
    "\n",
    "Note that some huggingface models cannot be fx traced directly and requires slight modification in modeling_[model].py. For example, for llama models, in transformers.models.llama.modeling_llama.py ```if query_states.device.type == \"cuda\" and causal_mask is not None:``` needs to be changed to ```if causal_mask is not None:```.\n",
    "\n",
    "If you use models from the d-matrix domain, changes in modeling_[model].py are made for tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmx.compressor.modeling.hf import pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"d-matrix/opt\",\n",
    "    revision=\"opt-125m\",\n",
    "    dmx_config=\"BASELINE\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",  # enabling model parallel on multi-GPU nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the model to formats equivalent to basic-mode execution on d-Matrix's hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmx.compressor import config_rules\n",
    "pipe.model = pipe.model.transform(\n",
    "    pipe.model.dmx_config,\n",
    "    *config_rules.BASIC,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones((1, 1024), dtype=int).to(\"cuda\")\n",
    "model_inputs = {\n",
    "    \"input_ids\": torch.tensor(\n",
    "        [[2, 11475, 2115, 10, 86, 11, 10, 1212, 444, 6, 444, 409]], device=\"cuda:0\"\n",
    "    ),\n",
    "    \"labels\": torch.tensor(\n",
    "        [[2, 11475, 2115, 10, 86, 11, 10, 1212, 444, 6, 444, 409]], device=\"cuda:0\"\n",
    "    ),\n",
    "    \"past_key_values\": None,\n",
    "    \"use_cache\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pipe.model(**model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure to other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmx.compressor.modeling import nn, DmxConfigRule\n",
    "bfp16 = \"BFP[8|8]{64}(SN)\"\n",
    "bfp14 = \"BFP[6|8]{64}(SN)\"\n",
    "rules = (\n",
    "    DmxConfigRule(\n",
    "        module_types=(nn.Embedding,),\n",
    "        module_config=dict(\n",
    "            input_formats=[bfp16],\n",
    "            output_format=bfp16,\n",
    "        ),\n",
    "    ),\n",
    "    DmxConfigRule(\n",
    "        module_types=(nn.Linear,),\n",
    "        module_config=dict(\n",
    "            input_format=[bfp16],\n",
    "            weight_format=bfp14,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model.configure(None, *rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check quantized GraphModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model._gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time in a land far, far away\"\n",
    "generated_texts = pipe(prompt, max_length=50, num_return_sequences=1)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unquantize and run text generation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = (\n",
    "    DmxConfigRule(\n",
    "        module_types=(nn.Embedding,),\n",
    "        module_config=dict(\n",
    "            input_formats=[\"SAME\"],\n",
    "            output_format=\"SAME\",\n",
    "        ),\n",
    "    ),\n",
    "    DmxConfigRule(\n",
    "        module_types=(nn.Linear,),\n",
    "        module_config=dict(\n",
    "            input_format=[\"SAME\"],\n",
    "            weight_format=\"SAME\",\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "pipe.model.configure(None, *rules)\n",
    "prompt = \"Once upon a time in a land far, far away\"\n",
    "generated_texts = pipe(prompt, max_length=50, num_return_sequences=1)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model._gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation on perplexity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = pipe.evaluate(\n",
    "    \"d-matrix/dmx_perplexity\",\n",
    "    dataset=\"wikitext\",\n",
    "    dataset_version=\"wikitext-2-raw-v1\",\n",
    ")\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
