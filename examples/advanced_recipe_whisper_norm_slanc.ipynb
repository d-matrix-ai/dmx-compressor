{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58301fd",
   "metadata": {},
   "source": [
    "## Example ADVANCED mode recipe - normalization layer extra parameters tuning by SLaNC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7acd8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'ABCMeta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdatasets\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mevaluate\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m load\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdmx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompressor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m DmxModel\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnormalize\u001b[39m(processor, text):\n\u001b[1;32m     13\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/dmx-compressor-internal/src/dmx/compressor/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtypes\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m SimpleNamespace\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdmx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompressor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m DmxModule\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mfunctools\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m partialmethod\n",
      "File \u001b[0;32m~/dmx-compressor-internal/src/dmx/compressor/modeling/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m DmxConfigRule, DmxModel\n\u001b[1;32m      4\u001b[0m \u001b[39m# NOTE: to be deprecated\u001b[39;00m\n",
      "File \u001b[0;32m~/dmx-compressor-internal/src/dmx/compressor/modeling/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mtorch_modules\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mcustom_modules\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/dmx-compressor-internal/src/dmx/compressor/modeling/nn/core.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfx\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Graph\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39minspect\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdmx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompressor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumerical\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m NumericalCastMixin, Same, CastTo\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdmx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompressor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     WeightSparseMixin,\n\u001b[1;32m     14\u001b[0m     Dense,\n\u001b[1;32m     15\u001b[0m     LazySparsify,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdmx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompressor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     ApproximationMixin,\n\u001b[1;32m     19\u001b[0m     NoApproximation,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/dmx-compressor-internal/src/dmx/compressor/numerical/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mformat\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Format, Same, NUMERICS_UTILS_AVAILABLE\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mcast\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m CastTo, CastToDict, NumericalCastMixin\n",
      "File \u001b[0;32m~/dmx-compressor-internal/src/dmx/compressor/numerical/format.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m BFPTypeEnum\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumerics\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m         determine_sbfp_scaler_exponent_bias_from_tensor_values,\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     NUMERICS_UTILS_AVAILABLE \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/common/refmodels/numerics/numerics/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumpy\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mformats\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdetermine_sbfp_scaler_exponent_bias_from_tensor_values\u001b[39m(x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m      9\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    This function determines SBFP exponent bias from a torch tensor instance\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/common/refmodels/numerics/numerics/formats.py:757\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m    756\u001b[0m \u001b[39m# SBFP does not inherit from Block format, since it has specific memory layout with precomputed exponent for BFP\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m \u001b[39mclass\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mSBFP12_16\u001b[39;00m(Format):\n\u001b[1;32m    758\u001b[0m     \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, ebias\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    759\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[39m        Construct empty SBFP matrix with specified block_size bits per element and ebias for scaling factor.\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[39m        Ebias = None means that ebias will be calculated when the data will be populated\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/common/refmodels/numerics/numerics/formats.py:800\u001b[0m, in \u001b[0;36mSBFP12_16\u001b[0;34m()\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[39massert\u001b[39;00m arr\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39m==\u001b[39m (\u001b[39m37\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[1;32m    797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m arr\n\u001b[0;32m--> 800\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconvert_from_float\u001b[39m(\u001b[39mself\u001b[39m, arr : np\u001b[39m.\u001b[39;49mndarray \u001b[39m|\u001b[39;49m FloatingPoint):\n\u001b[1;32m    801\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m    arr - np.array (float16 or float32) of two or more dimensions. The last two dimensions should be 64x64\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m    to reflect the tile layout of SBFP blocks.\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, FloatingPoint):\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'ABCMeta'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from dmx.compressor.modeling import DmxModel\n",
    "\n",
    "def normalize(processor, text):\n",
    "    try:\n",
    "        res = processor.tokenizer.normalize(text)\n",
    "    except:\n",
    "        res = text.lower().strip()\n",
    "    return res\n",
    "\n",
    "def run_evaluation(pipe, dataset_list, processor,wer_metric,eval_name):\n",
    "    \"\"\"Helper function to run evaluation and return predictions/references\"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    print(f\"Evaluating on {len(dataset_list)} samples...\")\n",
    "    \n",
    "    for i, sample in enumerate(dataset_list):\n",
    "        if i % 1 == 0:\n",
    "            print(f\"Processed {i}/{len(dataset_list)} samples\")\n",
    "\n",
    "        audio = sample[\"audio\"][\"array\"]\n",
    "        ground_truth = sample[\"text\"]\n",
    "\n",
    "        result = pipe(audio, return_timestamps=True)\n",
    "        prediction = result[\"text\"]\n",
    "\n",
    "        predictions.append(normalize(processor, prediction))\n",
    "        references.append(normalize(processor, ground_truth))\n",
    "    wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(f'***********{eval_name}\\n prediction: {predictions} \\n references: {references} \\n wer: {wer_score}')\n",
    "    return predictions, references , wer_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32146a0",
   "metadata": {},
   "source": [
    "\n",
    "1. Instantiate a `torch` model from source, HF hub in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65349518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 2 samples...\n",
      "Processed 0/2 samples\n",
      "Processed 1/2 samples\n",
      "***********vanilla\n",
      " prediction: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " references: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " wer: 0.0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "wer_metric = load(\"wer\")\n",
    "model_id = \"openai/whisper-medium\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model = model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "task = \"automatic-speech-recognition\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"librispeech_asr\", \"clean\", split=\"validation\", streaming=True, trust_remote_code=True\n",
    ")\n",
    "dataset = dataset.take(2)\n",
    "dataset_list = list(dataset)\n",
    "predictions_gt, references_gt, wer_gt = run_evaluation(pipe, dataset_list, processor, wer_metric, 'vanilla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1ea08",
   "metadata": {},
   "source": [
    "2. Transform into `DmxModel`; this does not change the functional behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78b5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 2 samples...\n",
      "Processed 0/2 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/2 samples\n",
      "***********baseline\n",
      " prediction: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " references: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " wer: 0.0\n"
     ]
    }
   ],
   "source": [
    "pipe.model = DmxModel.from_torch(pipe.model)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "predictions_baseline, references_baseline, wer_baseline = run_evaluation(pipe, dataset_list, processor, wer_metric, 'baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d57ed5",
   "metadata": {},
   "source": [
    "3. Configure to BASIC mode; this should bring in all VSIMD approximations with default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2de369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 2 samples...\n",
      "Processed 0/2 samples\n",
      "Processed 1/2 samples\n",
      "***********basic\n",
      " prediction: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " references: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " wer: 0.0\n"
     ]
    }
   ],
   "source": [
    "pipe.model.to_basic_mode()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "predictions_basic, references_basic, wer_basic = run_evaluation(pipe, dataset_list, processor, wer_metric, 'basic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db9d4f",
   "metadata": {},
   "source": [
    "4. SLaNC calibrate `LayerNorm` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e70d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLaNC done!\n",
      "Name: model.encoder.layers.0.self_attn_layer_norm, norm: {'norm': 1.0}\n",
      "Name: model.encoder.layers.0.final_layer_norm, norm: {'norm': tensor(0.0303, device='cuda:0')}\n",
      "Name: model.encoder.layers.1.self_attn_layer_norm, norm: {'norm': tensor(0.0300, device='cuda:0')}\n",
      "Name: model.encoder.layers.1.final_layer_norm, norm: {'norm': tensor(0.0395, device='cuda:0')}\n",
      "Name: model.encoder.layers.2.self_attn_layer_norm, norm: {'norm': tensor(0.0313, device='cuda:0')}\n",
      "Name: model.encoder.layers.2.final_layer_norm, norm: {'norm': tensor(0.0396, device='cuda:0')}\n",
      "Name: model.encoder.layers.3.self_attn_layer_norm, norm: {'norm': tensor(0.0302, device='cuda:0')}\n",
      "Name: model.encoder.layers.3.final_layer_norm, norm: {'norm': tensor(0.0414, device='cuda:0')}\n",
      "Name: model.encoder.layers.4.self_attn_layer_norm, norm: {'norm': tensor(0.0315, device='cuda:0')}\n",
      "Name: model.encoder.layers.4.final_layer_norm, norm: {'norm': tensor(0.0348, device='cuda:0')}\n",
      "Name: model.encoder.layers.5.self_attn_layer_norm, norm: {'norm': tensor(0.0329, device='cuda:0')}\n",
      "Name: model.encoder.layers.5.final_layer_norm, norm: {'norm': tensor(0.0342, device='cuda:0')}\n",
      "Name: model.encoder.layers.6.self_attn_layer_norm, norm: {'norm': tensor(0.0306, device='cuda:0')}\n",
      "Name: model.encoder.layers.6.final_layer_norm, norm: {'norm': tensor(0.0309, device='cuda:0')}\n",
      "Name: model.encoder.layers.7.self_attn_layer_norm, norm: {'norm': tensor(0.0357, device='cuda:0')}\n",
      "Name: model.encoder.layers.7.final_layer_norm, norm: {'norm': tensor(0.0338, device='cuda:0')}\n",
      "Name: model.encoder.layers.8.self_attn_layer_norm, norm: {'norm': tensor(0.0088, device='cuda:0')}\n",
      "Name: model.encoder.layers.8.final_layer_norm, norm: {'norm': tensor(0.0339, device='cuda:0')}\n",
      "Name: model.encoder.layers.9.self_attn_layer_norm, norm: {'norm': tensor(0.0292, device='cuda:0')}\n",
      "Name: model.encoder.layers.9.final_layer_norm, norm: {'norm': tensor(0.0331, device='cuda:0')}\n",
      "Name: model.encoder.layers.10.self_attn_layer_norm, norm: {'norm': tensor(0.0294, device='cuda:0')}\n",
      "Name: model.encoder.layers.10.final_layer_norm, norm: {'norm': tensor(0.0343, device='cuda:0')}\n",
      "Name: model.encoder.layers.11.self_attn_layer_norm, norm: {'norm': tensor(0.0294, device='cuda:0')}\n",
      "Name: model.encoder.layers.11.final_layer_norm, norm: {'norm': tensor(0.0312, device='cuda:0')}\n",
      "Name: model.encoder.layers.12.self_attn_layer_norm, norm: {'norm': tensor(0.0355, device='cuda:0')}\n",
      "Name: model.encoder.layers.12.final_layer_norm, norm: {'norm': tensor(0.0304, device='cuda:0')}\n",
      "Name: model.encoder.layers.13.self_attn_layer_norm, norm: {'norm': tensor(0.0373, device='cuda:0')}\n",
      "Name: model.encoder.layers.13.final_layer_norm, norm: {'norm': tensor(0.0289, device='cuda:0')}\n",
      "Name: model.encoder.layers.14.self_attn_layer_norm, norm: {'norm': tensor(0.0349, device='cuda:0')}\n",
      "Name: model.encoder.layers.14.final_layer_norm, norm: {'norm': tensor(0.0266, device='cuda:0')}\n",
      "Name: model.encoder.layers.15.self_attn_layer_norm, norm: {'norm': tensor(0.0361, device='cuda:0')}\n",
      "Name: model.encoder.layers.15.final_layer_norm, norm: {'norm': tensor(0.0269, device='cuda:0')}\n",
      "Name: model.encoder.layers.16.self_attn_layer_norm, norm: {'norm': tensor(0.0305, device='cuda:0')}\n",
      "Name: model.encoder.layers.16.final_layer_norm, norm: {'norm': tensor(0.0278, device='cuda:0')}\n",
      "Name: model.encoder.layers.17.self_attn_layer_norm, norm: {'norm': tensor(0.0224, device='cuda:0')}\n",
      "Name: model.encoder.layers.17.final_layer_norm, norm: {'norm': tensor(0.0233, device='cuda:0')}\n",
      "Name: model.encoder.layers.18.self_attn_layer_norm, norm: {'norm': tensor(0.0237, device='cuda:0')}\n",
      "Name: model.encoder.layers.18.final_layer_norm, norm: {'norm': tensor(0.0267, device='cuda:0')}\n",
      "Name: model.encoder.layers.19.self_attn_layer_norm, norm: {'norm': tensor(0.0266, device='cuda:0')}\n",
      "Name: model.encoder.layers.19.final_layer_norm, norm: {'norm': tensor(0.0261, device='cuda:0')}\n",
      "Name: model.encoder.layers.20.self_attn_layer_norm, norm: {'norm': tensor(0.0234, device='cuda:0')}\n",
      "Name: model.encoder.layers.20.final_layer_norm, norm: {'norm': tensor(0.0217, device='cuda:0')}\n",
      "Name: model.encoder.layers.21.self_attn_layer_norm, norm: {'norm': tensor(0.0155, device='cuda:0')}\n",
      "Name: model.encoder.layers.21.final_layer_norm, norm: {'norm': tensor(0.0218, device='cuda:0')}\n",
      "Name: model.encoder.layers.22.self_attn_layer_norm, norm: {'norm': tensor(0.0097, device='cuda:0')}\n",
      "Name: model.encoder.layers.22.final_layer_norm, norm: {'norm': tensor(0.0254, device='cuda:0')}\n",
      "Name: model.encoder.layers.23.self_attn_layer_norm, norm: {'norm': tensor(0.0095, device='cuda:0')}\n",
      "Name: model.encoder.layers.23.final_layer_norm, norm: {'norm': tensor(0.0200, device='cuda:0')}\n",
      "Name: model.encoder.layer_norm, norm: {'norm': tensor(0.0186, device='cuda:0')}\n",
      "Name: model.decoder.layers.0.self_attn_layer_norm, norm: {'norm': 1.0}\n",
      "Name: model.decoder.layers.0.encoder_attn_layer_norm, norm: {'norm': tensor(0.0285, device='cuda:0')}\n",
      "Name: model.decoder.layers.0.final_layer_norm, norm: {'norm': tensor(0.0509, device='cuda:0')}\n",
      "Name: model.decoder.layers.1.self_attn_layer_norm, norm: {'norm': tensor(0.0479, device='cuda:0')}\n",
      "Name: model.decoder.layers.1.encoder_attn_layer_norm, norm: {'norm': tensor(0.0887, device='cuda:0')}\n",
      "Name: model.decoder.layers.1.final_layer_norm, norm: {'norm': tensor(0.0282, device='cuda:0')}\n",
      "Name: model.decoder.layers.2.self_attn_layer_norm, norm: {'norm': tensor(0.0445, device='cuda:0')}\n",
      "Name: model.decoder.layers.2.encoder_attn_layer_norm, norm: {'norm': tensor(0.1205, device='cuda:0')}\n",
      "Name: model.decoder.layers.2.final_layer_norm, norm: {'norm': tensor(0.0439, device='cuda:0')}\n",
      "Name: model.decoder.layers.3.self_attn_layer_norm, norm: {'norm': tensor(0.0415, device='cuda:0')}\n",
      "Name: model.decoder.layers.3.encoder_attn_layer_norm, norm: {'norm': tensor(0.0857, device='cuda:0')}\n",
      "Name: model.decoder.layers.3.final_layer_norm, norm: {'norm': tensor(0.0331, device='cuda:0')}\n",
      "Name: model.decoder.layers.4.self_attn_layer_norm, norm: {'norm': tensor(0.0191, device='cuda:0')}\n",
      "Name: model.decoder.layers.4.encoder_attn_layer_norm, norm: {'norm': tensor(0.0476, device='cuda:0')}\n",
      "Name: model.decoder.layers.4.final_layer_norm, norm: {'norm': tensor(0.0346, device='cuda:0')}\n",
      "Name: model.decoder.layers.5.self_attn_layer_norm, norm: {'norm': tensor(0.0355, device='cuda:0')}\n",
      "Name: model.decoder.layers.5.encoder_attn_layer_norm, norm: {'norm': tensor(0.0309, device='cuda:0')}\n",
      "Name: model.decoder.layers.5.final_layer_norm, norm: {'norm': tensor(0.0204, device='cuda:0')}\n",
      "Name: model.decoder.layers.6.self_attn_layer_norm, norm: {'norm': tensor(0.0476, device='cuda:0')}\n",
      "Name: model.decoder.layers.6.encoder_attn_layer_norm, norm: {'norm': tensor(0.0390, device='cuda:0')}\n",
      "Name: model.decoder.layers.6.final_layer_norm, norm: {'norm': tensor(0.0291, device='cuda:0')}\n",
      "Name: model.decoder.layers.7.self_attn_layer_norm, norm: {'norm': tensor(0.0417, device='cuda:0')}\n",
      "Name: model.decoder.layers.7.encoder_attn_layer_norm, norm: {'norm': tensor(0.0295, device='cuda:0')}\n",
      "Name: model.decoder.layers.7.final_layer_norm, norm: {'norm': tensor(0.0198, device='cuda:0')}\n",
      "Name: model.decoder.layers.8.self_attn_layer_norm, norm: {'norm': tensor(0.0366, device='cuda:0')}\n",
      "Name: model.decoder.layers.8.encoder_attn_layer_norm, norm: {'norm': tensor(0.0313, device='cuda:0')}\n",
      "Name: model.decoder.layers.8.final_layer_norm, norm: {'norm': tensor(0.0203, device='cuda:0')}\n",
      "Name: model.decoder.layers.9.self_attn_layer_norm, norm: {'norm': tensor(0.0359, device='cuda:0')}\n",
      "Name: model.decoder.layers.9.encoder_attn_layer_norm, norm: {'norm': tensor(0.0337, device='cuda:0')}\n",
      "Name: model.decoder.layers.9.final_layer_norm, norm: {'norm': tensor(0.0227, device='cuda:0')}\n",
      "Name: model.decoder.layers.10.self_attn_layer_norm, norm: {'norm': tensor(0.0368, device='cuda:0')}\n",
      "Name: model.decoder.layers.10.encoder_attn_layer_norm, norm: {'norm': tensor(0.0253, device='cuda:0')}\n",
      "Name: model.decoder.layers.10.final_layer_norm, norm: {'norm': tensor(0.0233, device='cuda:0')}\n",
      "Name: model.decoder.layers.11.self_attn_layer_norm, norm: {'norm': tensor(0.0388, device='cuda:0')}\n",
      "Name: model.decoder.layers.11.encoder_attn_layer_norm, norm: {'norm': tensor(0.0253, device='cuda:0')}\n",
      "Name: model.decoder.layers.11.final_layer_norm, norm: {'norm': tensor(0.0367, device='cuda:0')}\n",
      "Name: model.decoder.layers.12.self_attn_layer_norm, norm: {'norm': tensor(0.0400, device='cuda:0')}\n",
      "Name: model.decoder.layers.12.encoder_attn_layer_norm, norm: {'norm': tensor(0.0286, device='cuda:0')}\n",
      "Name: model.decoder.layers.12.final_layer_norm, norm: {'norm': tensor(0.0321, device='cuda:0')}\n",
      "Name: model.decoder.layers.13.self_attn_layer_norm, norm: {'norm': tensor(0.0406, device='cuda:0')}\n",
      "Name: model.decoder.layers.13.encoder_attn_layer_norm, norm: {'norm': tensor(0.0259, device='cuda:0')}\n",
      "Name: model.decoder.layers.13.final_layer_norm, norm: {'norm': tensor(0.0254, device='cuda:0')}\n",
      "Name: model.decoder.layers.14.self_attn_layer_norm, norm: {'norm': tensor(0.0402, device='cuda:0')}\n",
      "Name: model.decoder.layers.14.encoder_attn_layer_norm, norm: {'norm': tensor(0.0228, device='cuda:0')}\n",
      "Name: model.decoder.layers.14.final_layer_norm, norm: {'norm': tensor(0.0272, device='cuda:0')}\n",
      "Name: model.decoder.layers.15.self_attn_layer_norm, norm: {'norm': tensor(0.0383, device='cuda:0')}\n",
      "Name: model.decoder.layers.15.encoder_attn_layer_norm, norm: {'norm': tensor(0.0222, device='cuda:0')}\n",
      "Name: model.decoder.layers.15.final_layer_norm, norm: {'norm': tensor(0.0290, device='cuda:0')}\n",
      "Name: model.decoder.layers.16.self_attn_layer_norm, norm: {'norm': tensor(0.0313, device='cuda:0')}\n",
      "Name: model.decoder.layers.16.encoder_attn_layer_norm, norm: {'norm': tensor(0.0224, device='cuda:0')}\n",
      "Name: model.decoder.layers.16.final_layer_norm, norm: {'norm': tensor(0.0241, device='cuda:0')}\n",
      "Name: model.decoder.layers.17.self_attn_layer_norm, norm: {'norm': tensor(0.0368, device='cuda:0')}\n",
      "Name: model.decoder.layers.17.encoder_attn_layer_norm, norm: {'norm': tensor(0.0245, device='cuda:0')}\n",
      "Name: model.decoder.layers.17.final_layer_norm, norm: {'norm': tensor(0.0253, device='cuda:0')}\n",
      "Name: model.decoder.layers.18.self_attn_layer_norm, norm: {'norm': tensor(0.0362, device='cuda:0')}\n",
      "Name: model.decoder.layers.18.encoder_attn_layer_norm, norm: {'norm': tensor(0.0230, device='cuda:0')}\n",
      "Name: model.decoder.layers.18.final_layer_norm, norm: {'norm': tensor(0.0266, device='cuda:0')}\n",
      "Name: model.decoder.layers.19.self_attn_layer_norm, norm: {'norm': tensor(0.0352, device='cuda:0')}\n",
      "Name: model.decoder.layers.19.encoder_attn_layer_norm, norm: {'norm': tensor(0.0248, device='cuda:0')}\n",
      "Name: model.decoder.layers.19.final_layer_norm, norm: {'norm': tensor(0.0273, device='cuda:0')}\n",
      "Name: model.decoder.layers.20.self_attn_layer_norm, norm: {'norm': tensor(0.0353, device='cuda:0')}\n",
      "Name: model.decoder.layers.20.encoder_attn_layer_norm, norm: {'norm': tensor(0.0262, device='cuda:0')}\n",
      "Name: model.decoder.layers.20.final_layer_norm, norm: {'norm': tensor(0.0265, device='cuda:0')}\n",
      "Name: model.decoder.layers.21.self_attn_layer_norm, norm: {'norm': tensor(0.0386, device='cuda:0')}\n",
      "Name: model.decoder.layers.21.encoder_attn_layer_norm, norm: {'norm': tensor(0.0235, device='cuda:0')}\n",
      "Name: model.decoder.layers.21.final_layer_norm, norm: {'norm': tensor(0.0262, device='cuda:0')}\n",
      "Name: model.decoder.layers.22.self_attn_layer_norm, norm: {'norm': tensor(0.0297, device='cuda:0')}\n",
      "Name: model.decoder.layers.22.encoder_attn_layer_norm, norm: {'norm': tensor(0.0246, device='cuda:0')}\n",
      "Name: model.decoder.layers.22.final_layer_norm, norm: {'norm': tensor(0.0237, device='cuda:0')}\n",
      "Name: model.decoder.layers.23.self_attn_layer_norm, norm: {'norm': tensor(0.0238, device='cuda:0')}\n",
      "Name: model.decoder.layers.23.encoder_attn_layer_norm, norm: {'norm': tensor(0.0294, device='cuda:0')}\n",
      "Name: model.decoder.layers.23.final_layer_norm, norm: {'norm': tensor(0.0287, device='cuda:0')}\n",
      "Name: model.decoder.layer_norm, norm: {'norm': tensor(0.0226, device='cuda:0')}\n",
      "Evaluating on 2 samples...\n",
      "Processed 0/2 samples\n",
      "Processed 1/2 samples\n",
      "***********basic_slanc\n",
      " prediction: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " references: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " wer: 0.0\n"
     ]
    }
   ],
   "source": [
    "from dmx.compressor import nn\n",
    "from dmx.compressor.modeling import DmxModule\n",
    "import re\n",
    "from dmx.compressor.advanced_recipe import (\n",
    "    DmxSLaNCHyperparams,\n",
    "    DmxSLaNCRecipe,\n",
    ")\n",
    "\n",
    "\n",
    "def hp_gen(_model) -> dict:\n",
    "    _hp = {}\n",
    "    complete_gm = list(_model._gms.values())[0]\n",
    "    named_dmx_modules = [(n,m) for (n,m) in complete_gm.named_modules() if isinstance(m, DmxModule)]\n",
    "\n",
    "    for _n, _m in named_dmx_modules:\n",
    "        if isinstance(_m, nn.LayerNorm):\n",
    "            if \".layer_norm\" in _n:\n",
    "                # final layer norm\n",
    "                layers = pipe.model.get_submodule(_n.replace(\".layer_norm\", \".layers\", -1))\n",
    "                layers = list(layers.children())\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"post_mlp\",\n",
    "                    mlp_type=\"standard\",\n",
    "                    device=_m.weight.device,\n",
    "                    prev_ln_weight=layers[-1].final_layer_norm,\n",
    "                    fc1=layers[-1].fc1,\n",
    "                    fc2=layers[-1].fc2\n",
    "                )\n",
    "            elif \"self_attn_layer_norm\" in _n and \".0.\" not in _n:\n",
    "                layer_num = int(''.join(re.findall(r'\\.\\d+\\.', _n)).replace(\".\", \"\", -1))\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"post_mlp\",\n",
    "                    mlp_type=\"standard\",\n",
    "                    device=_m.weight.device,\n",
    "                    prev_ln_weight=pipe.model.get_submodule(\n",
    "                        _n.replace(\"self_attn_layer_norm\", \"final_layer_norm\", -1)\n",
    "                        .replace(\".\" + str(layer_num) + \".\", \".\" + str(layer_num - 1) + \".\", -1)),\n",
    "                    fc1=pipe.model.get_submodule(\n",
    "                        _n.replace(\"self_attn_layer_norm\", \"fc1\", -1)\n",
    "                        .replace(\".\" + str(layer_num) + \".\", \".\" + str(layer_num - 1) + \".\", -1)),\n",
    "                    fc2=pipe.model.get_submodule(\n",
    "                        _n.replace(\"self_attn_layer_norm\", \"fc2\", -1)\n",
    "                        .replace(\".\" + str(layer_num) + \".\", \".\" + str(layer_num - 1) + \".\", -1))\n",
    "                )\n",
    "            elif \"encoder_attn_layer_norm\" in _n:\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"post_attn\",\n",
    "                    device=_m.weight.device,\n",
    "                    prev_ln_weight=pipe.model.get_submodule(\n",
    "                        _n.replace(\"encoder_attn_layer_norm\", \"self_attn_layer_norm\", -1)),\n",
    "                    v_proj=pipe.model.get_submodule(\n",
    "                        _n.replace(\"encoder_attn_layer_norm\", \"self_attn.v_proj\", -1)),\n",
    "                    o_proj=pipe.model.get_submodule(\n",
    "                        _n.replace(\"encoder_attn_layer_norm\", \"encoder_attn.out_proj\", -1))\n",
    "                )\n",
    "            elif \"final_layer_norm\" in _n:\n",
    "                if \".encoder.\" in _n:\n",
    "                    prev_ln_weight = pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"self_attn_layer_norm\", -1)\n",
    "                    )\n",
    "                    v_proj = pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"self_attn.v_proj\", -1)\n",
    "                    )\n",
    "                    o_proj = pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"self_attn.out_proj\", -1)\n",
    "                    )\n",
    "                elif \".decoder.\" in _n:\n",
    "                    prev_ln_weight = pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"encoder_attn_layer_norm\", -1)\n",
    "                    )\n",
    "                    v_proj=pipe.model.get_submodule(\n",
    "                        _n.replace(\".decoder.\", \".encoder.\", -1)\n",
    "                        .replace(\"final_layer_norm\", \"self_attn.v_proj\", -1)\n",
    "                    )\n",
    "                    o_proj=pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"encoder_attn.out_proj\", -1)\n",
    "                    )\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"post_attn\",\n",
    "                    device=_m.weight.device,\n",
    "                    prev_ln_weight=prev_ln_weight,\n",
    "                    v_proj=v_proj,\n",
    "                    o_proj=o_proj\n",
    "                )\n",
    "            elif \"self_attn_layer_norm\" in _n and \".0.\" in _n:\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"first\",\n",
    "                    device=_m.weight.device\n",
    "                )\n",
    "    return _hp\n",
    "\n",
    "with DmxSLaNCRecipe(hp_gen).applied_to(pipe.model):\n",
    "    print(\"SLaNC done!\")\n",
    "\n",
    "from dmx.compressor.modeling.model import DmxConfig\n",
    "complete_gm = list(pipe.model._gms.values())[0]\n",
    "\n",
    "all_modules_config  = DmxConfig({'_gm.'+n:m.dmx_config() for n,m in complete_gm.named_modules() if isinstance(m,DmxModule)})\n",
    "pipe.model.configure(all_modules_config)\n",
    " \n",
    "predictions_slanc,references_slanc,wer_slanc = run_evaluation(pipe,dataset_list,processor,wer_metric,'basic_slanc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cdc31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: model.encoder.layers.0.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.0.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.1.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.1.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.2.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.2.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.3.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.3.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.4.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.4.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.5.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.5.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.6.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.6.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.7.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.7.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.8.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.8.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.9.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.9.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.10.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.10.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.11.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.11.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.12.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.12.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.13.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.13.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.14.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.14.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.15.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.15.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.16.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.16.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.17.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.17.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.18.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.18.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.19.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.19.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.20.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.20.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.21.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.21.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.22.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.22.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.23.self_attn_layer_norm, norm: {}\n",
      "Name: model.encoder.layers.23.final_layer_norm, norm: {}\n",
      "Name: model.encoder.layer_norm, norm: {}\n",
      "Name: model.decoder.layers.0.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.0.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.0.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.1.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.1.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.1.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.2.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.2.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.2.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.3.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.3.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.3.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.4.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.4.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.4.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.5.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.5.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.5.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.6.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.6.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.6.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.7.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.7.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.7.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.8.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.8.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.8.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.9.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.9.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.9.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.10.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.10.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.10.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.11.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.11.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.11.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.12.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.12.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.12.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.13.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.13.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.13.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.14.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.14.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.14.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.15.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.15.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.15.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.16.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.16.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.16.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.17.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.17.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.17.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.18.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.18.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.18.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.19.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.19.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.19.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.20.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.20.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.20.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.21.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.21.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.21.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.22.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.22.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.22.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.23.self_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.23.encoder_attn_layer_norm, norm: {}\n",
      "Name: model.decoder.layers.23.final_layer_norm, norm: {}\n",
      "Name: model.decoder.layer_norm, norm: {}\n"
     ]
    }
   ],
   "source": [
    "complete_gm = list(pipe.model._gms.values())[0]\n",
    "named_dmx_modules = [(n,m) for (n,m) in complete_gm.named_modules() if isinstance(m, DmxModule)]\n",
    "\n",
    "for _n, _m in named_dmx_modules:\n",
    "    if isinstance(_m, nn.LayerNorm):\n",
    "        print(f\"Name: {_n}, norm: {_m.approximator.function.extra_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
