{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58301fd",
   "metadata": {},
   "source": [
    "## Example ADVANCED mode recipe - normalization layer extra parameters tuning by SLaNC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7acd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from dmx.compressor.modeling import DmxModel\n",
    "\n",
    "def normalize(processor, text):\n",
    "    try:\n",
    "        res = processor.tokenizer.normalize(text)\n",
    "    except:\n",
    "        res = text.lower().strip()\n",
    "    return res\n",
    "\n",
    "def run_evaluation(pipe, dataset_list, processor,wer_metric,eval_name):\n",
    "    \"\"\"Helper function to run evaluation and return predictions/references\"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    print(f\"Evaluating on {len(dataset_list)} samples...\")\n",
    "    \n",
    "    for i, sample in enumerate(dataset_list):\n",
    "        if i % 1 == 0:\n",
    "            print(f\"Processed {i}/{len(dataset_list)} samples\")\n",
    "\n",
    "        audio = sample[\"audio\"][\"array\"]\n",
    "        ground_truth = sample[\"text\"]\n",
    "\n",
    "        result = pipe(audio, return_timestamps=True)\n",
    "        prediction = result[\"text\"]\n",
    "\n",
    "        predictions.append(normalize(processor, prediction))\n",
    "        references.append(normalize(processor, ground_truth))\n",
    "    wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(f'***********{eval_name}\\n prediction: {predictions} \\n references: {references} \\n wer: {wer_score}')\n",
    "    return predictions, references , wer_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32146a0",
   "metadata": {},
   "source": [
    "\n",
    "1. Instantiate a `torch` model from source, HF hub in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65349518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 2 samples...\n",
      "Processed 0/2 samples\n",
      "Processed 1/2 samples\n",
      "***********vanilla\n",
      " prediction: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " references: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " wer: 0.0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "wer_metric = load(\"wer\")\n",
    "model_id = \"openai/whisper-medium\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model = model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "task = \"automatic-speech-recognition\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"librispeech_asr\", \"clean\", split=\"validation\", streaming=True, trust_remote_code=True\n",
    ")\n",
    "dataset = dataset.take(2)\n",
    "dataset_list = list(dataset)\n",
    "predictions_gt, references_gt, wer_gt = run_evaluation(pipe, dataset_list, processor, wer_metric, 'vanilla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1ea08",
   "metadata": {},
   "source": [
    "2. Transform into `DmxModel`; this does not change the functional behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea78b5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 2 samples...\n",
      "Processed 0/2 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/2 samples\n",
      "***********baseline\n",
      " prediction: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " references: ['he was in a fevered state of mind owing to the blight his wife is action threatened to cast upon his entire future', 'he would have to pay her the money which she would now regularly demand or there would be trouble it did not matter what he did'] \n",
      " wer: 0.0\n"
     ]
    }
   ],
   "source": [
    "pipe.model = DmxModel.from_torch(pipe.model)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "predictions_baseline, references_baseline, wer_baseline = run_evaluation(pipe, dataset_list, processor, wer_metric, 'baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d57ed5",
   "metadata": {},
   "source": [
    "3. Configure to BASIC mode; this should bring in all VSIMD approximations with default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2de369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 2 samples...\n",
      "Processed 0/2 samples\n"
     ]
    }
   ],
   "source": [
    "pipe.model.to_basic_mode()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "predictions_basic, references_basic, wer_basic = run_evaluation(pipe, dataset_list, processor, wer_metric, 'basic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db9d4f",
   "metadata": {},
   "source": [
    "4. SLaNC calibrate `LayerNorm` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmx.compressor import nn\n",
    "from dmx.compressor.modeling import DmxModule\n",
    "import re\n",
    "from dmx.compressor.advanced_recipe import (\n",
    "    DmxSLaNCHyperparams,\n",
    "    DmxSLaNCRecipe,\n",
    ")\n",
    "\n",
    "\n",
    "def hp_gen(_model) -> dict:\n",
    "    _hp = {}\n",
    "    complete_gm = list(_model._gms.values())[0]\n",
    "    named_dmx_modules = [(n,m) for (n,m) in complete_gm.named_modules() if isinstance(m, DmxModule)]\n",
    "\n",
    "    for _n, _m in named_dmx_modules:\n",
    "        if isinstance(_m, nn.LayerNorm):\n",
    "            if \".layer_norm\" in _n:\n",
    "                # final layer norm\n",
    "                layers = pipe.model.get_submodule(_n.replace(\".layer_norm\", \".layers\", -1))\n",
    "                layers = list(layers.children())\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"post_mlp\",\n",
    "                    mlp_type=\"standard\",\n",
    "                    device=_m.weight.device,\n",
    "                    prev_ln_weight=layers[-1].final_layer_norm,\n",
    "                    fc1=layers[-1].fc1,\n",
    "                    fc2=layers[-1].fc2\n",
    "                )\n",
    "            elif \"self_attn_layer_norm\" in _n and \".0.\" not in _n:\n",
    "                layer_num = int(''.join(re.findall(r'\\.\\d+\\.', _n)).replace(\".\", \"\", -1))\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"post_mlp\",\n",
    "                    mlp_type=\"standard\",\n",
    "                    device=_m.weight.device,\n",
    "                    prev_ln_weight=pipe.model.get_submodule(\n",
    "                        _n.replace(\"self_attn_layer_norm\", \"final_layer_norm\", -1)\n",
    "                        .replace(\".\" + str(layer_num) + \".\", \".\" + str(layer_num - 1) + \".\", -1)),\n",
    "                    fc1=pipe.model.get_submodule(\n",
    "                        _n.replace(\"self_attn_layer_norm\", \"fc1\", -1)\n",
    "                        .replace(\".\" + str(layer_num) + \".\", \".\" + str(layer_num - 1) + \".\", -1)),\n",
    "                    fc2=pipe.model.get_submodule(\n",
    "                        _n.replace(\"self_attn_layer_norm\", \"fc2\", -1)\n",
    "                        .replace(\".\" + str(layer_num) + \".\", \".\" + str(layer_num - 1) + \".\", -1))\n",
    "                )\n",
    "            elif \"encoder_attn_layer_norm\" in _n:\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"post_attn\",\n",
    "                    device=_m.weight.device,\n",
    "                    prev_ln_weight=pipe.model.get_submodule(\n",
    "                        _n.replace(\"encoder_attn_layer_norm\", \"self_attn_layer_norm\", -1)),\n",
    "                    v_proj=pipe.model.get_submodule(\n",
    "                        _n.replace(\"encoder_attn_layer_norm\", \"self_attn.v_proj\", -1)),\n",
    "                    o_proj=pipe.model.get_submodule(\n",
    "                        _n.replace(\"encoder_attn_layer_norm\", \"encoder_attn.out_proj\", -1))\n",
    "                )\n",
    "            elif \"final_layer_norm\" in _n:\n",
    "                if \".encoder.\" in _n:\n",
    "                    prev_ln_weight = pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"self_attn_layer_norm\", -1)\n",
    "                    )\n",
    "                    v_proj = pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"self_attn.v_proj\", -1)\n",
    "                    )\n",
    "                    o_proj = pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"self_attn.out_proj\", -1)\n",
    "                    )\n",
    "                elif \".decoder.\" in _n:\n",
    "                    prev_ln_weight = pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"encoder_attn_layer_norm\", -1)\n",
    "                    )\n",
    "                    v_proj=pipe.model.get_submodule(\n",
    "                        _n.replace(\".decoder.\", \".encoder.\", -1)\n",
    "                        .replace(\"final_layer_norm\", \"self_attn.v_proj\", -1)\n",
    "                    )\n",
    "                    o_proj=pipe.model.get_submodule(\n",
    "                        _n.replace(\"final_layer_norm\", \"encoder_attn.out_proj\", -1)\n",
    "                    )\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"post_attn\",\n",
    "                    device=_m.weight.device,\n",
    "                    prev_ln_weight=prev_ln_weight,\n",
    "                    v_proj=v_proj,\n",
    "                    o_proj=o_proj\n",
    "                )\n",
    "            elif \"self_attn_layer_norm\" in _n and \".0.\" in _n:\n",
    "                _hp[_m] = DmxSLaNCHyperparams(\n",
    "                    position=\"first\",\n",
    "                    device=_m.weight.device\n",
    "                )\n",
    "    return _hp\n",
    "\n",
    "with DmxSLaNCRecipe(hp_gen).applied_to(pipe.model):\n",
    "    print(\"SLaNC done!\")\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "complete_gm = list(pipe.model._gms.values())[0]\n",
    "named_dmx_modules = [(n,m) for (n,m) in complete_gm.named_modules() if isinstance(m, DmxModule)]\n",
    "\n",
    "for _n, _m in named_dmx_modules:\n",
    "    if isinstance(_m, nn.LayerNorm):\n",
    "        print(f\"Name: {_n}, norm: {_m.approximator.function.extra_params}\")\n",
    "predictions_slanc,references_slanc,wer_slanc = run_evaluation(pipe,dataset_list,processor,wer_metric,'basic_slanc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cdc31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_gm = list(pipe.model._gms.values())[0]\n",
    "named_dmx_modules = [(n,m) for (n,m) in complete_gm.named_modules() if isinstance(m, DmxModule)]\n",
    "\n",
    "for _n, _m in named_dmx_modules:\n",
    "    if isinstance(_m, nn.LayerNorm):\n",
    "        print(f\"Name: {_n}, norm: {_m.approximator.function.extra_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
