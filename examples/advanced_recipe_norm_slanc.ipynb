{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58301fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example ADVANCED mode recipe - normalization layer extra parameters tuning by SLaNC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7acd8a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32146a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "1. Instantiate a `torch` model from source, HF hub in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65349518",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.9988455772399902, 'label': 'a photo of cats'},\n",
       "  {'score': 0.0011544461594894528, 'label': 'a photo of dogs'}],\n",
       " [{'score': 0.9962840676307678, 'label': 'a kitchen scene'},\n",
       "  {'score': 0.0037159069906920195, 'label': 'a living room scene'}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import CLIPModel\n",
    "model = \"openai/clip-vit-base-patch32\"\n",
    "task = \"zero-shot-image-classification\"\n",
    "\n",
    "task_cases = [\n",
    "    dict(\n",
    "        images=\"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "        candidate_labels=[\n",
    "            \"a photo of cats\",\n",
    "            \"a photo of dogs\",\n",
    "        ],\n",
    "    ),\n",
    "    dict(\n",
    "        images=\"http://images.cocodataset.org/val2017/000000397133.jpg\",\n",
    "        candidate_labels=[\n",
    "            \"a kitchen scene\",\n",
    "            \"a living room scene\",\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "[pipe(**_tc) for _tc in task_cases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1ea08",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Transform into `DmxModel`; this does not change the functional behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea78b5a4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.9988455772399902, 'label': 'a photo of cats'},\n",
       "  {'score': 0.0011544550070539117, 'label': 'a photo of dogs'}],\n",
       " [{'score': 0.9962841868400574, 'label': 'a kitchen scene'},\n",
       "  {'score': 0.0037158718332648277, 'label': 'a living room scene'}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dmx.compressor import DmxModel\n",
    "\n",
    "pipe.model = DmxModel.from_torch(pipe.model)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "[pipe(**_tc) for _tc in task_cases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d57ed5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Configure to BASIC mode; this should bring in all VSIMD approximations with default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2de369",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.9199306964874268, 'label': 'a photo of cats'},\n",
       "  {'score': 0.08006926625967026, 'label': 'a photo of dogs'}],\n",
       " [{'score': 0.9805154800415039, 'label': 'a kitchen scene'},\n",
       "  {'score': 0.019484540447592735, 'label': 'a living room scene'}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.to_basic_mode()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "[pipe(**_tc) for _tc in task_cases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db9d4f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. SLaNC calibrate `LayerNorm` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288e70d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLaNC done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.9986134767532349, 'label': 'a photo of cats'},\n",
       "  {'score': 0.0013864935608580709, 'label': 'a photo of dogs'}],\n",
       " [{'score': 0.9974502921104431, 'label': 'a kitchen scene'},\n",
       "  {'score': 0.002549670170992613, 'label': 'a living room scene'}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dmx.compressor import nn\n",
    "import re\n",
    "from dmx.compressor.advanced_recipe import (\n",
    "    DmxSLaNCHyperparams,\n",
    "    DmxSLaNCRecipe,\n",
    ")\n",
    "\n",
    "def get_clip_slanc_layers(model):\n",
    "    assert model.class_for_deserialization == CLIPModel    \n",
    "    _hp = {}\n",
    "    n_layers = len(model.text_model.encoder.layers)\n",
    "    assert n_layers == len(model.vision_model.encoder.layers)\n",
    "    for layer_stack in (model._gm.text_model.encoder.layers,model._gm.vision_model.encoder.layers):\n",
    "        for idx in range(n_layers):\n",
    "            #Keep the first lnorm layer in the stack at default scale\n",
    "            if idx > 0:\n",
    "                _hp[layer_stack.get_submodule(str(idx)).layer_norm1] = DmxSLaNCHyperparams(position=\"post_mlp\",\n",
    "                                                                        mlp_type=\"standard\",\n",
    "                                                                        device=layer_stack.get_submodule(str(idx)).layer_norm1.weight.device,\n",
    "                                                                        prev_ln_weight=layer_stack.get_submodule(str(idx - 1)).layer_norm2,\n",
    "                                                                        fc1=layer_stack.get_submodule(str(idx - 1)).mlp.fc1,\n",
    "                                                                        fc2=layer_stack.get_submodule(str(idx - 1)).mlp.fc2\n",
    "                )\n",
    "            else:\n",
    "                _hp[layer_stack.get_submodule(str(idx)).layer_norm1] = DmxSLaNCHyperparams(position=\"first\",\n",
    "                                                                        device=layer_stack.get_submodule(str(idx)).layer_norm1.weight.device\n",
    "                                                                        )\n",
    "            _hp[layer_stack.get_submodule(str(idx)).layer_norm2] = DmxSLaNCHyperparams(position=\"post_attn\",\n",
    "                                                                        device=layer_stack.get_submodule(str(idx)).layer_norm2.weight.device,\n",
    "                                                                        prev_ln_weight=layer_stack.get_submodule(str(idx)).layer_norm1,\n",
    "                                                                        v_proj=layer_stack.get_submodule(str(idx)).self_attn.v_proj,\n",
    "                                                                        o_proj=layer_stack.get_submodule(str(idx)).self_attn.out_proj,\n",
    "                                                                        )\n",
    "    #special cases\n",
    "    _hp[model._gm.vision_model.pre_layrnorm] = DmxSLaNCHyperparams(position=\"first\",\n",
    "                                                                device=model._gm.vision_model.pre_layrnorm.weight.device\n",
    "                                                                )\n",
    "    _hp[model._gm.text_model.final_layer_norm] = DmxSLaNCHyperparams(position=\"post_mlp\",\n",
    "                                                                mlp_type=\"standard\",\n",
    "                                                                device=model._gm.text_model.final_layer_norm.weight.device,\n",
    "                                                                prev_ln_weight=model.text_model.encoder.layers[-1].layer_norm2,\n",
    "                                                                fc1=model.text_model.encoder.layers[-1].mlp.fc1,\n",
    "                                                                fc2=model.text_model.encoder.layers[-1].mlp.fc2\n",
    "                                                                )\n",
    "\n",
    "    _hp[model._gm.vision_model.post_layernorm] = DmxSLaNCHyperparams(position=\"post_mlp\",\n",
    "                                                                mlp_type=\"standard\",\n",
    "                                                                device=model._gm.vision_model.post_layernorm.weight.device,\n",
    "                                                                prev_ln_weight=model.vision_model.encoder.layers[-1].layer_norm2,\n",
    "                                                                fc1=model.vision_model.encoder.layers[-1].mlp.fc1,\n",
    "                                                                fc2=model.vision_model.encoder.layers[-1].mlp.fc2\n",
    "                                                                )\n",
    "    return _hp\n",
    "\n",
    "\n",
    "def hp_gen(_model) -> dict:\n",
    "    if _model.class_for_deserialization == CLIPModel:\n",
    "        return get_clip_slanc_layers(_model)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model class for extracting SLANC layers: {_model.class_for_deserialization}')\n",
    "    \n",
    "\n",
    "with DmxSLaNCRecipe(hp_gen).applied_to(pipe.model):\n",
    "   print(\"SLaNC done!\")\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "[pipe(**_tc) for _tc in task_cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919db4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: vision_model.pre_layrnorm, norm: {'norm': 1.0}\n",
      "Name: vision_model.encoder.layers.0.layer_norm1, norm: {'norm': 1.0}\n",
      "Name: vision_model.encoder.layers.0.layer_norm2, norm: {'norm': tensor(0.0564, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.1.layer_norm1, norm: {'norm': tensor(0.0298, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.1.layer_norm2, norm: {'norm': tensor(0.0383, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.2.layer_norm1, norm: {'norm': tensor(0.1633, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.2.layer_norm2, norm: {'norm': tensor(0.0413, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.3.layer_norm1, norm: {'norm': tensor(0.2988, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.3.layer_norm2, norm: {'norm': tensor(0.0334, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.4.layer_norm1, norm: {'norm': tensor(0.2830, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.4.layer_norm2, norm: {'norm': tensor(0.0310, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.5.layer_norm1, norm: {'norm': tensor(0.2542, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.5.layer_norm2, norm: {'norm': tensor(0.0303, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.6.layer_norm1, norm: {'norm': tensor(0.1924, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.6.layer_norm2, norm: {'norm': tensor(0.0306, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.7.layer_norm1, norm: {'norm': tensor(0.1218, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.7.layer_norm2, norm: {'norm': tensor(0.0284, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.8.layer_norm1, norm: {'norm': tensor(0.0979, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.8.layer_norm2, norm: {'norm': tensor(0.0255, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.9.layer_norm1, norm: {'norm': tensor(0.0462, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.9.layer_norm2, norm: {'norm': tensor(0.0224, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.10.layer_norm1, norm: {'norm': tensor(0.0192, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.10.layer_norm2, norm: {'norm': tensor(0.0204, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.11.layer_norm1, norm: {'norm': tensor(0.0195, device='cuda:0')}\n",
      "Name: vision_model.encoder.layers.11.layer_norm2, norm: {'norm': tensor(0.0215, device='cuda:0')}\n",
      "Name: vision_model.post_layernorm, norm: {'norm': tensor(0.0621, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.0.layer_norm1, norm: {'norm': 1.0}\n",
      "Name: text_model.encoder.layers.0.layer_norm2, norm: {'norm': tensor(0.0209, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.1.layer_norm1, norm: {'norm': tensor(0.0426, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.1.layer_norm2, norm: {'norm': tensor(0.0313, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.2.layer_norm1, norm: {'norm': tensor(0.1391, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.2.layer_norm2, norm: {'norm': tensor(0.0279, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.3.layer_norm1, norm: {'norm': tensor(0.1397, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.3.layer_norm2, norm: {'norm': tensor(0.0278, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.4.layer_norm1, norm: {'norm': tensor(0.1413, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.4.layer_norm2, norm: {'norm': tensor(0.0265, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.5.layer_norm1, norm: {'norm': tensor(0.1369, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.5.layer_norm2, norm: {'norm': tensor(0.0248, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.6.layer_norm1, norm: {'norm': tensor(0.1267, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.6.layer_norm2, norm: {'norm': tensor(0.0248, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.7.layer_norm1, norm: {'norm': tensor(0.1261, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.7.layer_norm2, norm: {'norm': tensor(0.0231, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.8.layer_norm1, norm: {'norm': tensor(0.1196, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.8.layer_norm2, norm: {'norm': tensor(0.0212, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.9.layer_norm1, norm: {'norm': tensor(0.1071, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.9.layer_norm2, norm: {'norm': tensor(0.0216, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.10.layer_norm1, norm: {'norm': tensor(0.1040, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.10.layer_norm2, norm: {'norm': tensor(0.0186, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.11.layer_norm1, norm: {'norm': tensor(0.1029, device='cuda:0')}\n",
      "Name: text_model.encoder.layers.11.layer_norm2, norm: {'norm': tensor(0.0173, device='cuda:0')}\n",
      "Name: text_model.final_layer_norm, norm: {'norm': tensor(0.0998, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "from dmx.compressor.modeling import DmxModule\n",
    "\n",
    "complete_gm = list(pipe.model._gms.values())[0]\n",
    "named_dmx_modules = [(n,m) for (n,m) in complete_gm.named_modules() if isinstance(m, DmxModule)]\n",
    "for _n, _m in named_dmx_modules:\n",
    "    if isinstance(_m, nn.LayerNorm):\n",
    "        print(f\"Name: {_n}, norm: {_m.approximator.function.extra_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "name": "advanced_recipe_norm_slanc.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
