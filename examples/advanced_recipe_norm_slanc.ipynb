{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58301fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example ADVANCED mode recipe - normalization layer extra parameters tuning by SLaNC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7acd8a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32146a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "1. Instantiate a `torch` model from source, HF hub in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65349518",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.9988459348678589, 'label': 'a photo of cats'},\n",
       "  {'score': 0.0011540568666532636, 'label': 'a photo of dogs'}],\n",
       " [{'score': 0.9962789416313171, 'label': 'a kitchen scene'},\n",
       "  {'score': 0.0037210225127637386, 'label': 'a living room scene'}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import CLIPModel\n",
    "model = \"openai/clip-vit-base-patch32\"\n",
    "task = \"zero-shot-image-classification\"\n",
    "\n",
    "task_cases = [\n",
    "    dict(\n",
    "        images=\"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "        candidate_labels=[\n",
    "            \"a photo of cats\",\n",
    "            \"a photo of dogs\",\n",
    "        ],\n",
    "    ),\n",
    "    dict(\n",
    "        images=\"http://images.cocodataset.org/val2017/000000397133.jpg\",\n",
    "        candidate_labels=[\n",
    "            \"a kitchen scene\",\n",
    "            \"a living room scene\",\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=task,\n",
    "    model=model,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "[pipe(**_tc) for _tc in task_cases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1ea08",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Transform into `DmxModel`; this does not change the functional behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea78b5a4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.9988459348678589, 'label': 'a photo of cats'},\n",
       "  {'score': 0.0011540524428710341, 'label': 'a photo of dogs'}],\n",
       " [{'score': 0.9962789416313171, 'label': 'a kitchen scene'},\n",
       "  {'score': 0.003721001325175166, 'label': 'a living room scene'}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dmx.compressor import DmxModel\n",
    "\n",
    "pipe.model = DmxModel.from_torch(pipe.model)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "[pipe(**_tc) for _tc in task_cases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d57ed5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Configure to BASIC mode; this should bring in all VSIMD approximations with default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2de369",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.843508780002594, 'label': 'a photo of cats'},\n",
       "  {'score': 0.1564912348985672, 'label': 'a photo of dogs'}],\n",
       " [{'score': 0.938322126865387, 'label': 'a kitchen scene'},\n",
       "  {'score': 0.061677876859903336, 'label': 'a living room scene'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.to_basic_mode()\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "[pipe(**_tc) for _tc in task_cases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db9d4f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. SLaNC calibrate `LayerNorm` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288e70d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLaNC done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.8301814794540405, 'label': 'a photo of cats'},\n",
       "  {'score': 0.16981850564479828, 'label': 'a photo of dogs'}],\n",
       " [{'score': 0.9074352979660034, 'label': 'a kitchen scene'},\n",
       "  {'score': 0.092564657330513, 'label': 'a living room scene'}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dmx.compressor import nn\n",
    "from dmx.compressor.advanced_recipe import (\n",
    "    DmxSLaNCHyperparams,\n",
    "    DmxSLaNCRecipe,\n",
    ")\n",
    "\n",
    "def get_clip_slanc_layers(model):\n",
    "    assert model.class_for_deserialization == CLIPModel    \n",
    "    _hp = {}\n",
    "    n_layers = len(model.text_model.encoder.layers)\n",
    "    assert n_layers == len(model.vision_model.encoder.layers)\n",
    "    for layer_stack in (model._gm.text_model.encoder.layers,model._gm.vision_model.encoder.layers):\n",
    "        for idx in range(n_layers):\n",
    "            #Keep the first lnorm layer in the stack at default scale\n",
    "            if idx > 0:\n",
    "                _hp[layer_stack.get_submodule(str(idx)).layer_norm1] = DmxSLaNCHyperparams(position= \"post_mlp\",\n",
    "                                                                        prev_layer= layer_stack.get_submodule(str(idx-1)).mlp,\n",
    "                                                                        prev_ln_weight= layer_stack.get_submodule(str(idx-1)).layer_norm2.weight\n",
    "                                                                        )\n",
    "            _hp[layer_stack.get_submodule(str(idx)).layer_norm2] = DmxSLaNCHyperparams(position=\"post_attn\",\n",
    "                                                         prev_layer=layer_stack.get_submodule(str(idx)).self_attn,\n",
    "                                                         prev_ln_weight=layer_stack.get_submodule(str(idx)).layer_norm1.weight\n",
    "                                                         )\n",
    "    #special cases\n",
    "    _hp[model._gm.text_model.final_layer_norm] = DmxSLaNCHyperparams(position=\"post_mlp\",\n",
    "                                                                 prev_layer=model.text_model.encoder.layers[-1].mlp,\n",
    "                                                                 prev_ln_weight=model.text_model.encoder.layers[-1].layer_norm2.weight\n",
    "                                                                 )\n",
    "\n",
    "    _hp[model._gm.vision_model.post_layernorm] = DmxSLaNCHyperparams(position=\"post_mlp\",\n",
    "                                                                 prev_layer=model.vision_model.encoder.layers[-1].mlp,\n",
    "                                                                 prev_ln_weight=model.vision_model.encoder.layers[-1].layer_norm2.weight\n",
    "                                                                 )\n",
    "\n",
    "\n",
    "    return _hp\n",
    "\n",
    "def hp_gen(_model) -> dict:\n",
    "    if _model.class_for_deserialization == CLIPModel:\n",
    "        return get_clip_slanc_layers(_model)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model class for extracting SLANC layers: {_model.class_for_deserialization}')\n",
    "    \n",
    "\n",
    "with DmxSLaNCRecipe(hp_gen).applied_to(pipe.model):\n",
    "   print(\"SLaNC done!\")\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "[pipe(**_tc) for _tc in task_cases]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "name": "advanced_recipe_norm_slanc.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
