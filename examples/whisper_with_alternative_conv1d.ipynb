{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get whisper to `dmx.compiler` with alternative `Conv1d` subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original inference\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\", \n",
    "    model=model_id,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "pipe(\"audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add these additional lines before transformation/configuration to map Conv1d to our alternative version\n",
    "from dmx.compressor.modeling.nn import experimental\n",
    "from dmx.compressor.fx.transformer.utils import dmx_aware_mapping\n",
    "\n",
    "dmx_aware_mapping[\"torch.nn.modules.conv.Conv1d\"] = experimental.Conv1d\n",
    "\n",
    "# ---\n",
    "# transformation and configuration\n",
    "from dmx.compressor import DmxModel\n",
    "\n",
    "pipe.model = DmxModel.from_torch(pipe.model)\n",
    "pipe.model.to_basic_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model.dmx_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a forward that triggers JIT tracing through encoder\n",
    "inputs = {\n",
    "    \"input_features\": torch.rand(1, 80, 3000).to(\"cuda\"),\n",
    "    \"decoder_input_ids\": torch.tensor([[50258]], device=\"cuda\"),\n",
    "}\n",
    "output = pipe.model(**inputs)\n",
    "\n",
    "# QdQ transform pipe.model._gm and show there is no conv1d in encoder\n",
    "from dmx.compressor.fx.transform import make_compiler_graph\n",
    "import dmir_compiler\n",
    "compiler_graph = make_compiler_graph(pipe.model._gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model._gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler_graph.graph.print_tabular()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
