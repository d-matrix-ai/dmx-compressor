{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmx.compressor.modeling.hf import DmxModel\n",
    "import torch\n",
    "\n",
    "\n",
    "class Submod(torch.nn.Module):\n",
    "    def __init__(self, indim, hiddim, outdim) -> None:\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(indim, hiddim)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.lin2 = torch.nn.Linear(hiddim, outdim)\n",
    "\n",
    "    def forward(self, x, y, relu = True):\n",
    "        out = self.lin1(x + y)\n",
    "        if relu:\n",
    "            out = self.act(out)\n",
    "        out = self.lin2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sm1 = Submod(160, 6400, 6400)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.sm2 = Submod(6400, 12800, 10)\n",
    "\n",
    "    def forward(self, x, relu = True):\n",
    "        out = self.sm1(x, x, relu)\n",
    "        out = self.act(out)\n",
    "        out = self.sm2(out, out, relu)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output of the main model and submodule is different from baseline model after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = CustomModel()\n",
    "model = CustomModel()\n",
    "model.load_state_dict(model0.state_dict())\n",
    "model = DmxModel.from_torch(model)\n",
    "inp = torch.rand((1, 160))\n",
    "ref_output = model0(inp)\n",
    "assert torch.allclose(model(inp),model0(inp))\n",
    "model.to_basic_mode()\n",
    "assert not torch.allclose(model(inp),model0(inp))\n",
    "assert not torch.allclose(model0.sm1(inp,inp),model.sm1(inp,inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output of the whole model is equivalent to running submodules sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basic_output = model(inp)\n",
    "assert not torch.allclose(ref_output, basic_output)\n",
    "basic_output_from_submod = model.sm1(inp, inp)\n",
    "basic_output_from_submod = model.act(basic_output_from_submod)\n",
    "basic_output_from_submod = model.sm2(basic_output_from_submod, basic_output_from_submod)\n",
    "torch.allclose(basic_output, basic_output_from_submod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DmxModules are shared accross _gm for different model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.sm1._gm.lin1 is model._gm.sm1.lin1\n",
    "assert model.act._gm is model._gm.act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the model to baseline, output of the whole model is still equivalent to running submodules sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_baseline_mode()\n",
    "baseline_output = model(inp)\n",
    "assert torch.allclose(ref_output, baseline_output)\n",
    "baseline_output_from_submod = model.sm1(inp, inp)\n",
    "baseline_output_from_submod = model.act(baseline_output_from_submod)\n",
    "baseline_output_from_submod = model.sm2(baseline_output_from_submod, baseline_output_from_submod)\n",
    "torch.allclose(baseline_output, baseline_output_from_submod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change in control flow triggers submod retransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(model0.sm1(inp,inp, False),model.sm1(inp,inp, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizing submodules changes output of main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sm1.to_basic_mode()\n",
    "assert not torch.allclose(model0.sm1(inp,inp, False),model.sm1(inp,inp, False))\n",
    "assert not torch.allclose(model(inp),model0(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "from dmx.compressor.modeling.hf import DmxModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model0 = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-tiny\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda\",\n",
    "        attn_implementation=\"eager\",\n",
    "    ).to(\"cuda\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-tiny\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda\",\n",
    "        attn_implementation=\"eager\",\n",
    "    ).to(\"cuda\")\n",
    "model = DmxModel.from_torch(model)\n",
    "\n",
    "dataset = list(\n",
    "        load_dataset(\n",
    "            \"librispeech_asr\",\n",
    "            \"clean\",\n",
    "            split=\"validation\",\n",
    "            streaming=True,\n",
    "        )\n",
    "    )\n",
    "audio_sample = dataset[0][\"audio\"]\n",
    "input_features = processor(\n",
    "    audio_sample[\"array\"],\n",
    "    sampling_rate=audio_sample[\"sampling_rate\"],\n",
    "    return_tensors=\"pt\",\n",
    ").input_features\n",
    "input_features = input_features.to(\"cuda\", dtype=torch.float16)\n",
    "decoder_input_ids = torch.randint(0, 100, (1, 2)).to(\"cuda\")\n",
    "ref_output = model0(input_features, decoder_input_ids=decoder_input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(ref_output.logits,model(input_features, decoder_input_ids=decoder_input_ids).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmx.compressor.modeling.model import *\n",
    "bfp14 = \"BFP[6|8]{64}(SN)\"\n",
    "rules = (\n",
    "    DmxConfigRule(\n",
    "        module_types=(Linear,),\n",
    "        module_config=dict(\n",
    "            weight_format=bfp14,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "model.configure(None, *rules)\n",
    "assert not torch.allclose(model(input_features, decoder_input_ids=decoder_input_ids).logits,ref_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not torch.allclose(model.model.encoder(input_features).last_hidden_state,model0.model.encoder(input_features).last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder_last_hidden_state of quantized main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_features, decoder_input_ids=decoder_input_ids).encoder_last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder_last_hidden_state of unquantized HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output.encoder_last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder_last_hidden_state of quantized encoder only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.encoder(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model0 = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", attn_implementation=\"eager\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "model0.eval()\n",
    "outputs_ref = model0(**inputs)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = DmxModel.from_torch(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(outputs_ref.logits_per_image, model(**inputs).logits_per_image)\n",
    "model.to_basic_mode()\n",
    "assert not torch.allclose(outputs_ref.logits_per_image, model(**inputs).logits_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_baseline_mode()\n",
    "assert torch.allclose(model.text_model(inputs['input_ids']).pooler_output,model0.text_model(inputs['input_ids']).pooler_output,atol=1e-6)\n",
    "assert torch.allclose(model.vision_model(inputs['pixel_values']).pooler_output,model0.vision_model(inputs['pixel_values']).pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_basic_mode()\n",
    "assert not torch.allclose(model.text_model(inputs['input_ids']).pooler_output,model0.text_model(inputs['input_ids']).pooler_output,atol=1e-3)\n",
    "assert not torch.allclose(model.vision_model(inputs['pixel_values']).pooler_output,model0.vision_model(inputs['pixel_values']).pooler_output,atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "from dmx.compressor.modeling import DmxModel\n",
    "model_name = \"d-matrix/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float, device_map=\"cuda\",trust_remote_code=True\n",
    ")\n",
    "model0 = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float, device_map=\"cuda\", attn_implementation=\"eager\",trust_remote_code=True)\n",
    "model = DmxModel.from_torch(model)\n",
    "model.eval()\n",
    "model0.eval()\n",
    "input_ids = torch.randint(0, 100, (1, 8)).to(\"cuda:0\")\n",
    "with torch.no_grad():\n",
    "    submod_input = model0.model(input_ids).last_hidden_state\n",
    "    model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.full(\n",
    "    (1, 1, 8, model.config.max_position_embeddings),\n",
    "    fill_value=-torch.inf,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "position_ids = torch.range(0, input_ids.shape[1] - 1).unsqueeze(0).to('cuda')\n",
    "with torch.no_grad():\n",
    "    ref_output = model0.model.layers[1](submod_input, position_ids=position_ids, attention_mask=causal_mask)[0]\n",
    "    output = model.model.layers[1](submod_input, position_ids=position_ids, attention_mask=causal_mask)[0]\n",
    "assert torch.allclose(ref_output, output,1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmx.compressor.modeling.model import *\n",
    "bfp14 = \"BFP[6|8]{64}(SN)\"\n",
    "rules = (\n",
    "    DmxConfigRule(\n",
    "        module_types=(Linear,),\n",
    "        module_config=dict(\n",
    "            weight_format=bfp14,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "model.configure(None, *rules)\n",
    "with torch.no_grad():\n",
    "    ref_output = model0.model.layers[1](submod_input, position_ids=position_ids, attention_mask=causal_mask)[0]\n",
    "    output = model.model.layers[1](submod_input, position_ids=position_ids, attention_mask=causal_mask)[0]\n",
    "assert not torch.allclose(ref_output, output,1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
