{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch nightly and transformers==4.52.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dmx.compressor.modeling import DmxModel, nn\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_opt():\n",
    "    model_name = \"d-matrix/opt-125m\"\n",
    "    model1 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    model2 = AutoModelForCausalLM.from_pretrained(\n",
    "        \"facebook/opt-125m\",\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    model_inputs = {k: v.to(\"cuda:0\") for k, v in model1.dummy_inputs.items()}\n",
    "    input_ids = torch.randint(0, 100, (3, 8)).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        output0 = model1(**model_inputs)\n",
    "\n",
    "        generated_ids_0 = model1.generate(input_ids, max_new_tokens=5, do_sample=False)\n",
    "        # compiled(**model_inputs)\n",
    "\n",
    "        model2 = DmxModel.from_torch(model2, export=True)\n",
    "\n",
    "        output2 = model2(**model_inputs)\n",
    "\n",
    "        assert torch.allclose(output0.logits, output2.logits)\n",
    "\n",
    "        model2(input_ids)\n",
    "        generated_ids2 = model2.generate(input_ids, max_new_tokens=5, do_sample=False)\n",
    "        assert torch.allclose(generated_ids_0, generated_ids2)\n",
    "\n",
    "        model2.to_basic_mode()\n",
    "\n",
    "        quant_output2 = model2(**model_inputs)\n",
    "        quant_generated_ids2 = model2.generate(\n",
    "            input_ids, max_new_tokens=5, do_sample=False\n",
    "        )\n",
    "\n",
    "        model1 = DmxModel.from_torch(model1)\n",
    "\n",
    "        output1 = model1(**model_inputs)\n",
    "\n",
    "        generated_ids1 = model1.generate(input_ids, max_new_tokens=5, do_sample=False)\n",
    "\n",
    "        model1.to_basic_mode()\n",
    "\n",
    "        quant_output1 = model1(**model_inputs)\n",
    "        quant_generated_ids1 = model1.generate(\n",
    "            input_ids, max_new_tokens=5, do_sample=False\n",
    "        )\n",
    "    assert torch.allclose(output0.logits, output1.logits)\n",
    "    assert torch.allclose(output2.logits, output1.logits)\n",
    "    assert torch.allclose(generated_ids_0, generated_ids1)\n",
    "    assert torch.allclose(generated_ids2, generated_ids1)\n",
    "    assert not torch.allclose(quant_output1.logits, output0.logits)\n",
    "    assert torch.allclose(quant_output1.logits, quant_output2.logits)\n",
    "    assert not torch.allclose(generated_ids_0, quant_generated_ids1)\n",
    "    assert torch.allclose(quant_generated_ids2, quant_generated_ids1)\n",
    "\n",
    "test_opt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt quantized submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_opt_submod():\n",
    "    model_name = \"d-matrix/opt-125m\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model1 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    model2 = AutoModelForCausalLM.from_pretrained(\n",
    "        \"facebook/opt-125m\",\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    input_ids = torch.randint(0, 100, (3, 8)).to(\"cuda:0\")\n",
    "    hidden_states = torch.rand(3, 8, 768).to(\"cuda\").to(torch.float16)\n",
    "    attention_mask = torch.ones(3, 1, 8, 8).to(\"cuda\").to(torch.float16)\n",
    "    with torch.no_grad():\n",
    "        output0 = model1(input_ids)\n",
    "        decoder_output0 = model1.model.decoder(input_ids).last_hidden_state\n",
    "        decoder_layer_output0 = model1.model.decoder.layers[0](\n",
    "            hidden_states, attention_mask=attention_mask\n",
    "        )[0]\n",
    "\n",
    "        model2 = DmxModel.from_torch(model2, export=True)\n",
    "        output2 = model2(input_ids)\n",
    "\n",
    "        decoder_output2 = model2.model.decoder(input_ids).last_hidden_state\n",
    "        decoder_layer_output2 = model2.model.decoder.layers[0](\n",
    "            hidden_states, attention_mask=attention_mask\n",
    "        )[0]\n",
    "\n",
    "        assert torch.allclose(output0.logits, output2.logits)\n",
    "        assert torch.allclose(decoder_output0, decoder_output2)\n",
    "        assert torch.allclose(decoder_layer_output0, decoder_layer_output2)\n",
    "\n",
    "        model2.to_basic_mode()\n",
    "\n",
    "        quant_decoder_output2 = model2.model.decoder(input_ids).last_hidden_state\n",
    "        quant_decoder_layer_output2 = model2.model.decoder.layers[0](\n",
    "            hidden_states, attention_mask=attention_mask\n",
    "        )[0]\n",
    "\n",
    "        model1 = DmxModel.from_torch(model1)\n",
    "\n",
    "        output1 = model1(input_ids)\n",
    "        decoder_output1 = model1.model.decoder(input_ids).last_hidden_state\n",
    "        decoder_layer_output1 = model1.model.decoder.layers[0](\n",
    "            hidden_states, attention_mask=attention_mask\n",
    "        )[0]\n",
    "\n",
    "        model1.to_basic_mode()\n",
    "\n",
    "        quant_decoder_output1 = model1.model.decoder(input_ids).last_hidden_state\n",
    "        quant_decoder_layer_output1 = model1.model.decoder.layers[0](\n",
    "            hidden_states, attention_mask=attention_mask\n",
    "        )[0]\n",
    "    assert torch.allclose(output0.logits, output1.logits)\n",
    "    assert torch.allclose(output2.logits, output1.logits)\n",
    "    assert torch.allclose(decoder_output0, decoder_output1)\n",
    "    assert torch.allclose(decoder_output2, decoder_output1)\n",
    "    assert torch.allclose(decoder_layer_output0, decoder_layer_output1)\n",
    "    assert torch.allclose(decoder_layer_output2, decoder_layer_output1)\n",
    "\n",
    "    assert not torch.allclose(quant_decoder_output1, decoder_output0)\n",
    "    assert torch.allclose(quant_decoder_output1, quant_decoder_output2)\n",
    "    assert not torch.allclose(quant_decoder_layer_output1, decoder_layer_output0)\n",
    "    assert torch.allclose(quant_decoder_layer_output1, quant_decoder_layer_output2)\n",
    "\n",
    "test_opt_submod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipe():\n",
    "    from transformers import pipeline\n",
    "    from dmx.compressor import DmxModel\n",
    "\n",
    "    model = \"facebook/opt-125m\"\n",
    "    task = \"text-generation\"\n",
    "\n",
    "    task_cases = [\n",
    "        dict(\n",
    "            text_inputs=\"Once upon a time,\",\n",
    "        ),\n",
    "        dict(\n",
    "            text_inputs=\"To be honest,\",\n",
    "        ),\n",
    "    ]\n",
    "    from transformers.generation import GenerationConfig\n",
    "\n",
    "    pipe = pipeline(\n",
    "        task=task,\n",
    "        model=model,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        generation_config=GenerationConfig(\n",
    "            use_cache=True,\n",
    "            cache_implementation=\"static\",\n",
    "            max_length=1024,\n",
    "            cache_config={\n",
    "                \"batch_size\": 1,\n",
    "                \"max_cache_len\": 1024,\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    out0 = [pipe(**_tc, do_sample=False) for _tc in task_cases]\n",
    "\n",
    "\n",
    "    pipe.model = DmxModel.from_torch(pipe.model, export=True)\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    torch.manual_seed(42)\n",
    "    out = [pipe(**_tc, do_sample=False) for _tc in task_cases]\n",
    "    assert out[0][0][\"generated_text\"] == out0[0][0][\"generated_text\"]\n",
    "    assert out[1][0][\"generated_text\"] == out0[1][0][\"generated_text\"]\n",
    "\n",
    "test_pipe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt2():\n",
    "    model_name = \"d-matrix/gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model1 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    model2 = AutoModelForCausalLM.from_pretrained(\n",
    "        \"openai-community/gpt2\",\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    model_inputs = {k: v.to(\"cuda:0\") for k, v in model1.dummy_inputs.items()}\n",
    "    input_ids = torch.randint(0, 100, (3, 8)).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        output0 = model1(**model_inputs)\n",
    "\n",
    "        generated_ids_0 = model1.generate(input_ids, max_new_tokens=5, do_sample=False)\n",
    "        # compiled(**model_inputs)\n",
    "\n",
    "        model2 = DmxModel.from_torch(model2, export=True)\n",
    "\n",
    "        output2 = model2(**model_inputs)\n",
    "\n",
    "        assert torch.allclose(output0.logits, output2.logits)\n",
    "\n",
    "        model2(input_ids)\n",
    "        generated_ids2 = model2.generate(input_ids, max_new_tokens=5, do_sample=False)\n",
    "        assert torch.allclose(generated_ids_0, generated_ids2)\n",
    "\n",
    "        model2.to_basic_mode()\n",
    "\n",
    "        quant_output2 = model2(**model_inputs)\n",
    "        quant_generated_ids2 = model2.generate(\n",
    "            input_ids, max_new_tokens=5, do_sample=False\n",
    "        )\n",
    "\n",
    "        model1 = DmxModel.from_torch(model1)\n",
    "\n",
    "        output1 = model1(**model_inputs)\n",
    "\n",
    "        model1.to_basic_mode()\n",
    "\n",
    "        quant_output1 = model1(**model_inputs)\n",
    "\n",
    "    assert torch.allclose(output0.logits, output1.logits)\n",
    "    assert torch.allclose(output2.logits, output1.logits)\n",
    "    assert torch.allclose(generated_ids_0, generated_ids2)\n",
    "    assert not torch.allclose(quant_output1.logits, output0.logits)\n",
    "    assert torch.allclose(quant_output1.logits, quant_output2.logits)\n",
    "    assert not torch.allclose(generated_ids_0, quant_generated_ids2)\n",
    "test_gpt2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_whisper():\n",
    "    # pip install datasets\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "    import librosa\n",
    "\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-tiny\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda\",\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "    task = \"automatic-speech-recognition\"\n",
    "    pipe = pipeline(\n",
    "        task=task,\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "    )\n",
    "    audio, sr = librosa.load(\"audio.mp3\", sr=16000)\n",
    "    input_features = processor(\n",
    "        audio,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_features\n",
    "\n",
    "    input_features = input_features.to(\"cuda\", dtype=torch.float16)\n",
    "    decoder_input_ids = torch.randint(0, 100, (1, 2)).to(\"cuda\")\n",
    "    out0 = model(input_features, decoder_input_ids=decoder_input_ids)\n",
    "    generation0 = pipe(\"audio.mp3\", return_timestamps=True)\n",
    "    model = DmxModel.from_torch(model, export=True)\n",
    "    out1 = model(input_features, decoder_input_ids=decoder_input_ids)\n",
    "    generation1_nocache = model.generate(\n",
    "        input_features, decoder_input_ids=decoder_input_ids, use_cache=False\n",
    "    )\n",
    "    generation1 = pipe(\"audio.mp3\", return_timestamps=True)\n",
    "    assert generation0[\"text\"] == generation1[\"text\"]\n",
    "test_whisper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper smoothquant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_whisper_smoothquant():\n",
    "    import torch\n",
    "    from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-tiny\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda\",\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    model0 = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-tiny\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda\",\n",
    "        attn_implementation=\"sdpa\",\n",
    "    ).to(\"cuda\")\n",
    "    processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "    task = \"automatic-speech-recognition\"\n",
    "\n",
    "    pipe = pipeline(\n",
    "        task=task,\n",
    "        model=model0,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "    )\n",
    "    pipe0 = pipeline(\n",
    "        task=task,\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "    out0 = pipe(\"audio.mp3\")\n",
    "    pipe.model = DmxModel.from_torch(pipe.model, export=True)\n",
    "    pipe0.model = DmxModel.from_torch(pipe0.model, export=True)\n",
    "\n",
    "    pipe(\"audio.mp3\", generate_kwargs={\"do_sample\": False})\n",
    "    pipe0(\"audio.mp3\", generate_kwargs={\"do_sample\": False})\n",
    "\n",
    "    from dmx.compressor import nn\n",
    "\n",
    "    from dmx.compressor.advanced_recipe import (\n",
    "        DmxModuleSmoothQuantHyperparams,\n",
    "        DmxSmoothQuantRecipe,\n",
    "    )\n",
    "\n",
    "    def hp_gen(_model) -> dict:\n",
    "        return {\n",
    "            _m: DmxModuleSmoothQuantHyperparams(\n",
    "                migration_strength=0.25,\n",
    "                fuse_to_weight=True,\n",
    "            )\n",
    "            for _n, _m in _model.named_dmx_modules()\n",
    "            if \"decoder\" in _n and isinstance(_m, (nn.Linear,)) and \"proj_out\" not in _n\n",
    "        }\n",
    "\n",
    "    def hp_gen_no_fuse(_model) -> dict:\n",
    "        return {\n",
    "            _m: DmxModuleSmoothQuantHyperparams(\n",
    "                migration_strength=0.25,\n",
    "                fuse_to_weight=False,\n",
    "            )\n",
    "            for _n, _m in _model.named_dmx_modules()\n",
    "            if isinstance(_m, (nn.Linear,)) and \"proj_out\" not in _n\n",
    "        }\n",
    "\n",
    "    with DmxSmoothQuantRecipe(hp_gen_no_fuse).applied_to(pipe0.model):\n",
    "        pipe0(\n",
    "            \"audio.mp3\", generate_kwargs={\"do_sample\": False}\n",
    "        )\n",
    "    out1 = pipe(\"audio.mp3\", generate_kwargs={\"do_sample\": False})\n",
    "    with DmxSmoothQuantRecipe(hp_gen).applied_to(pipe.model):\n",
    "        pipe(\n",
    "            \"audio.mp3\", generate_kwargs={\"do_sample\": False}\n",
    "        )\n",
    "    out2 = pipe(\"audio.mp3\", generate_kwargs={\"do_sample\": False})\n",
    "    assert out0[\"text\"] == out1[\"text\"]\n",
    "    assert out1[\"text\"] == out2[\"text\"]\n",
    "\n",
    "    pipe0.model.to_basic_mode()\n",
    "    pipe.model.to_basic_mode()\n",
    "    out3 = pipe0(\"audio.mp3\", generate_kwargs={\"do_sample\": False})\n",
    "    out4 = pipe(\"audio.mp3\", generate_kwargs={\"do_sample\": False})\n",
    "    assert not out0[\"text\"] == out3[\"text\"]\n",
    "    assert out3[\"text\"] == out4[\"text\"]\n",
    "\n",
    "test_whisper_smoothquant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clip():\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    import torch\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    from dmx.compressor.modeling import DmxModel\n",
    "\n",
    "    model1 = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    model2 = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    output0 = model1(**inputs)\n",
    "\n",
    "    model1 = DmxModel.from_torch(model1, export=False)\n",
    "    model2 = DmxModel.from_torch(model2, export=True)\n",
    "\n",
    "    output1 = model1(**inputs)\n",
    "    output2 = model2(**inputs)\n",
    "    assert torch.allclose(output0.logits_per_image, output1.logits_per_image)\n",
    "    assert torch.allclose(output1.logits_per_image, output2.logits_per_image)\n",
    "\n",
    "    model1.to_basic_mode()\n",
    "    model2.to_basic_mode()\n",
    "    output1_quant = model1(**inputs)\n",
    "    output2_quant = model2(**inputs)\n",
    "    assert not torch.allclose(output0.logits_per_image, output1_quant.logits_per_image)\n",
    "    assert torch.allclose(\n",
    "        output2_quant.logits_per_image, output1_quant.logits_per_image\n",
    "    )\n",
    "test_clip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qwen():\n",
    "    model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float, device_map=\"cuda\"\n",
    "    )\n",
    "    model.eval()\n",
    "    input_ids = torch.randint(0, 100, (3, 8)).to(\"cuda:0\")\n",
    "    ref_output = model(input_ids)\n",
    "    model = DmxModel.from_torch(model, export=True)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids)\n",
    "    assert torch.allclose(ref_output.logits, out.logits, atol=1e-4)\n",
    "    model.to_basic_mode()\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids)\n",
    "    assert not torch.allclose(ref_output.logits, out.logits, atol=1e-4)\n",
    "test_qwen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_llama3():\n",
    "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "    # model_name = \"facebook/opt-125m\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float, device_map=\"cuda\"\n",
    "    )\n",
    "    model.eval()\n",
    "    input_ids = torch.randint(0, 100, (3, 8)).to(\"cuda:0\")\n",
    "    ref_output = model(input_ids)\n",
    "    model = DmxModel.from_torch(model, export=True)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids)\n",
    "    assert torch.allclose(ref_output.logits, out.logits, atol=1e-4)\n",
    "    model.to_basic_mode()\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids)\n",
    "    assert not torch.allclose(ref_output.logits, out.logits, atol=1e-4)\n",
    "test_llama3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
